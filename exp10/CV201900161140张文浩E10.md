## 姓名学号

- 张文浩 201900161140

## 班级

- 19级人工智能班

## 实验题目

- 图像拼接

## 实验要求

- 基于OpenCV实现图像拼接，可以对两张或更多的输入图像，将图像对齐后拼接成一张全景图。对影响拼接效果的各种因素（特征匹配、相机位移、场景几何等）拍摄图像进行测试。

## 实验原理

> #### 图像拼接的基本步骤

概括来说就是：

1. 读取图像（需要按顺序）
2. 找到图像间的逻辑一致性（单应性）
3. 拼接图像

接下来将从第二点开始对图像拼接的原理进行阐述。

> #### 图像拼接的原理

- **Homography 单应性**

  因为图片拼接的过程中，图片需要进行一些单应性变换才可以进行较好的拼接。是发生在投影平面上的点和线的投影映射。使用在图像拼接中，通过单应性变化，把图片投影成拼接需要的形状。

  - 单应性矩阵

    单应性矩阵主要解决两个问题

    1. 表述真实世界中一个平面与对应它图像的透视变换
    2. 从通过透视变换实现图像从一种视图变换到另外一种视图

  $$
  \left[\begin{matrix}x^,_i\\y^,_i\\w^,\\\end{matrix}\right]=\left[\begin{matrix}h_1&h_2&h_3\\h_4&h_5&h_6\\h_7&h_8&h_9\\\end{matrix}\right]\left[\begin{matrix}x\\y\\w\\\end{matrix}\right]
  $$

  ​	点的齐次坐标依赖于其尺度定义，因此单应性矩阵H也仅依赖尺度定义，所以，单应性矩阵具有8个独立的自由度。

  - **单应性变化**

    opencv中计算单应矩阵的函数为**findHomography**

    ```python
    Mat cv::findHomography	(	InputArray 	srcPoints,
                                    InputArray 	dstPoints,
                                    int 	method = 0,
                                    double 	ransacReprojThreshold = 3,
                                    OutputArray 	mask = noArray(),
                                    const int 	maxIters = 2000,
                                    const double 	confidence = 0.995 
                            )
    ```

    计算多个二维点对之间的最优单映射变换矩阵 H（3行x3列） ，使用最小均方误差或者RANSAC方法

    - 函数功能：找到两个平面之间的转换矩阵。
    - 参数详解：

    | 参数                  |                                                              |
    | --------------------- | ------------------------------------------------------------ |
    | srcPoints             | 源平面中点的坐标矩阵，可以是CV_32FC2类型，也可以是vector<Point2f>类型 |
    | dstPoints             | 目标平面中点的坐标矩阵，可以是CV_32FC2类型，也可以是vector<Point2f>类型 |
    | method                | 计算单应矩阵所使用的方法。不同的方法对应不同的参数，具体如下：    **0** - 利用所有点的常规方法                                                                 **RANSAC** - RANSAC-基于RANSAC的鲁棒算法                                     **LMEDS** - 最小中值鲁棒算法                                                                          **RHO** - PROSAC-基于PROSAC的鲁棒算法 |
    | ransacReprojThreshold | 将点对视为内点的最大允许重投影错误阈值（仅用于RANSAC和RHO方法）。如果![img](https://img-blog.csdnimg.cn/20190220172146988.png)则点![img](https://img-blog.csdnimg.cn/20190220172218876.png)被认为是个外点（即错误匹配点对）。若srcPoints和dstPoints是以像素为单位的，则该参数通常设置在1到10的范围内。 |
    | mask                  | 可选输出掩码矩阵，通常由鲁棒算法（RANSAC或LMEDS）设置。 请注意，输入掩码矩阵是不需要设置的。 |
    | maxIters              | RANSAC算法的最大迭代次数，默认值为2000。                     |
    | confidence            | 可信度值，取值范围为0到1.                                    |

    该函数能够找到并返回源平面和目标平面之间的变换矩阵H，以便于反向投影错误率达到最小。
    $$
    s_i\left[\begin{matrix}x^,_i\\y^,_i\\1\\\end{matrix}\right]——>
    H\left[\begin{matrix}x_i\\y_i\\1\\\end{matrix}\right]
    $$
    ​	如果不是所有的点对（srcPointsi，dstPointsi）都适合刚性透视变换（也就是说，存在一些外点（匹配错误点对）），那么这个初始估计就会很差。 在这种情况下，您可以使用三种鲁棒方法之一。 RANSAC，LMeDS和RHO等算法尝试对应点对（每对四对）的许多不同随机子集，使用该子集和简单最小二乘算法估计单应矩阵，然后计算计算单应性的质量/良好性（对于**RANSAC**来说，单应性的质量是指内点的数量，对于LMeDs来说，单应性的质量是指中值重投影的误差）。 最后，使用最佳子集来产生单应矩阵的初始估计和内点/异常值的掩模。

在计算单应性之前，需要进行特征提取和特征匹配：

- 特征提取:

  特征提取使用是SIFT算子

- 图片的匹配:

  图片的匹配可以采用OpenCV提供的FLANN或BFMatcher

计算完单应性后就得到了变换矩阵，下一步就根据变换矩阵对图像进行变形（透视变换），然后拼接

- **Warping & Stitching 翘曲和拼接**

  1.翘曲（Warping）
  	在建立完单应性之后，我们就会知道拼接图片在被拼接图片的视角上看应该是怎么样的情况。在这种情况下，我们需要对图片进行一定程度的变形，即翘曲。
  	翘曲有三种类型：Planar（平面，即旋转和平移）、Cylindrical（圆柱，即把图像绘制在一个圆柱表面上）、Spherical （球形，和圆柱是同类概念）。
  	我们可以使用单应性矩阵来完成，在代码中表示为：

  ```python
  warped_image = cv2.warpPerspective(image, homography_matrix,dimension_of_warped_image)
  ```

  2.拼接（Stitching）

  ​	在翘曲完成后，通过重复对扭曲的图像向左向右缝合即可完成图像的拼接。

  ​	对于图像拼接部分的原理大致可以表示为如下过程：如果一个图像开始的坐标点为( 0 , 0 ) ，结束的坐标点为( r_e , c_e )，那我们可以通过翘曲得到一个新的图片矩阵，从开始点start：H × [ 0 , 0 ] 到结束点end：H × [ r e , c e ]，如果开始点为负数，就对它进行平移。并且还要确保单应性矩阵的归一化。然后再使用basic for looping constructs和覆盖两个图像的方法对图像进行拼接，方法的输入将是stableimage和warpedImage。迭代两个图像对其进行处理，如果像素相等，则将像素作为该值。否则优先考虑非黑色像素。

## 实验步骤

- 定义一个类matchers，用来得到两个图像之间的变换矩阵。

- 步骤如下：

  1. 先利用SURF对两张图像进行特征提取
  2. 利用flann对两张图像计算出来的特征描述符进行匹配
  3. 清除错误匹配，将误差大于一定值的舍去
  4. 因为H变换矩阵中有8个未知参数，求解的话至少需要4对匹配的点，所以只有在正确匹配数量大于4的情况下，才会去计算H变换矩阵。

  关键代码如下：

```python
	def match(self, i1, i2, direction=None):
        #计算i1，i2图像之间的变换矩阵H，先利用SURF对两张图像进行特征提取
		imageSet1 = self.getSURFFeatures(i1)
		imageSet2 = self.getSURFFeatures(i2)
		print("Direction : ", direction) #向左拼接还是向右拼接
        #利用flann对两张图像计算出来的特征描述符进行匹配
		matches = self.flann.knnMatch(
			imageSet2['des'],
			imageSet1['des'],
			k=2
			)
        #清除错误匹配，将误差大于一定值的舍去
		good = []
		for i , (m, n) in enumerate(matches):
			if m.distance < 0.7*n.distance:
				good.append((m.trainIdx, m.queryIdx))

		if len(good) > 4:
			pointsCurrent = imageSet2['kp']
			pointsPrevious = imageSet1['kp']
            #提取出匹配的点的坐标
			matchedPointsCurrent = np.float32(
				[pointsCurrent[i].pt for (__, i) in good]
			)
			matchedPointsPrev = np.float32(
				[pointsPrevious[i].pt for (i, __) in good]
				)
            # 使用findHomography函数，计算多个二维点对之间的最优单映射变换矩阵 H（3行x3列） ，使用最小均方误差或者RANSAC方法
			H, s = cv2.findHomography(matchedPointsCurrent, matchedPointsPrev, cv2.RANSAC, 4)
			return H
		return None

	def getSURFFeatures(self, im):
        #获得surf特征检测
		gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
        #kp是特征检测出来的关键点
		kp, des = self.surf.detectAndCompute(gray, None)
		return {'kp':kp, 'des':des}
```

​	有了计算H特征矩阵的函数之后，进行主函数的构建。

​	因为实验要求要实现多张图片进行拼接，而不只是两张图片拼接，所以考虑到图像拼接时要进行变换操作，很可能导致图像严重变形，如果我们以一端为基准点，在向另一端进行拼接的时候，图像变形会原来越严重，当图片数量稍微大一点的时候就会导致效果很差，所以我们在用的方法是以中间的图像为及基准，即最终得到的图像最中间的那一张是不变形的，也就是说，中心图像左边的所有图像都会向右拼接；中心图像右边的所有图像都会向左拼接，这样两边的图像变形不会太过严重。

- 构造一个将一系列图像分为左右两部分的函数

```python
	def prepare_lists(self):
        #将一系列图像分为左右两部分，左边的部分向左拼接，右边的部分向右拼接
		print("Number of images : %d"%self.count)
		self.centerIdx = self.count/2 
		print("Center index image : %d"%self.centerIdx)
		self.center_im = self.images[int(self.centerIdx)]
		for i in range(self.count):
			if(i<=self.centerIdx):
				self.left_list.append(self.images[i])
			else:
				self.right_list.append(self.images[i])
```

- 将左边的图像拼接（左边的图像进行形变）

  ​	对于图像拼接部分的原理大致可以表示为如下过程：如果一个图像开始的坐标点为( 0 , 0 ) ，结束的坐标点为( r_e , c_e )，那我们可以通过翘曲得到一个新的图片矩阵，从开始点start：H × [ 0 , 0 ] 到结束点end：H × [ r e , c e ]，如果开始点为负数，就对它进行平移。并且还要确保单应性矩阵的归一化。

```python
def leftshift(self):
    #左边的部分向左拼接（通过变换左边的矩阵进行拼接，所以要取变换矩阵H的逆）
	a = self.left_list[0]
	for b in self.left_list[1:]:
        #H就是得到的从a到b的变化矩阵
		H = self.matcher_obj.match(a, b, 'left')
		xh = np.linalg.inv(H)
		ds = np.dot(xh, np.array([a.shape[1], a.shape[0], 1]));
		ds = ds/ds[-1]
        #f1表示图像a的start_p，如果开始点为负数，就对它进行平移
		f1 = np.dot(xh, np.array([0,0,1]))
		f1 = f1/f1[-1]
		xh[0][-1] += abs(f1[0])
		xh[1][-1] += abs(f1[1])
		ds = np.dot(xh, np.array([a.shape[1], a.shape[0], 1]))
		offsety = abs(int(f1[1]))
		offsetx = abs(int(f1[0]))
		dsize = (int(ds[0])+offsetx, int(ds[1]) + offsety)
        #透视变换，然后拼接
		tmp = cv2.warpPerspective(a, xh, dsize)
		tmp[offsety:b.shape[0]+offsety, offsetx:b.shape[1]+offsetx] = b
		a = tmp
	self.leftImage = tmp
```

- 同理，对右边的所有图像向左拼接

```python
def rightshift(self):
    #右边的部分向右拼接
	for each in self.right_list:
		H = self.matcher_obj.match(self.leftImage, each, 'right')
		txyz = np.dot(H, np.array([each.shape[1], each.shape[0], 1]))
		txyz = txyz/txyz[-1]
		dsize =(int(txyz[0])+self.leftImage.shape[1],
                int(txyz[1])+self.leftImage.shape[0])
		tmp = cv2.warpPerspective(each, H, dsize)
		cv2.imshow("tp", tmp)
		cv2.waitKey()
        #这里需要将左边已经拼接好的图像与有点刚进行透视变换的图像进行拼接
		tmp = self.mix_and_match(self.leftImage, tmp)
		self.leftImage = tmp
```

- 使用basic for looping constructs和覆盖两个图像的方法对图像进行拼接，方法的输入将是stableimage和warpedImage。迭代两个图像对其进行处理，如果像素相等，则将像素作为该值。否则优先考虑非黑色像素。

```python
	def mix_and_match(self, leftImage, warpedImage):
		i1y, i1x = leftImage.shape[:2]
		i2y, i2x = warpedImage.shape[:2]

		black_l = np.where(leftImage == np.array([0,0,0]))
		for i in range(0, i1x):
			for j in range(0, i1y):
				try:
					if(np.array_equal(leftImage[j,i],np.array([0,0,0])) and  
                       np.array_equal(warpedImage[j,i],np.array([0,0,0]))):
                        #如果像素值相等，就将像素作为该值
						warpedImage[j,i] = [0, 0, 0]
					else:#否则优先考虑黑色像素
						if(np.array_equal(warpedImage[j,i],[0,0,0])):
							warpedImage[j,i] = leftImage[j,i]
						else:
							if not np.array_equal(leftImage[j,i], [0,0,0]):
								bw, gw, rw = warpedImage[j,i]
								bl,gl,rl = leftImage[j,i]
								# b = (bl+bw)/2
								# g = (gl+gw)/2
								# r = (rl+rw)/2
								warpedImage[j, i] = [bl,gl,rl]
				except:
					pass
		return warpedImage #返回拼接好的图像
```

## 实验结果

先用实验要求里给的学校大门的三张图做一下测试，效果如图所示：

<img src="G:\Tableau\image\image-20211126231157365.png" alt="image-20211126231157365" style="zoom: 40%;" />

整体效果还可以，但是拼接的地方还是可以看出没有对齐的地方。

下面根据实验要求测试影响拼接效果的各种因素。

- **特征匹配影响**

上面的图是通过surf算子计算特征点的，surf检测检测出来的特征点如下：

<img src="G:\Tableau\image\image-20211126231742098.png" alt="image-20211126231742098" style="zoom:33%;" /><img src="G:\Tableau\image\image-20211126231804172.png" alt="image-20211126231804172" style="zoom:33%;" /><img src="G:\Tableau\image\image-20211126231827538.png" alt="image-20211126231827538" style="zoom:33%;" />

<img src="G:\Tableau\image\image-20211126231157365.png" alt="image-20211126231157365" style="zoom: 40%;" />

如果换成sift算子进行特征检测，效果如下：

<img src="G:\Tableau\image\image-20211126231945867.png" alt="image-20211126231945867" style="zoom:33%;" /><img src="G:\Tableau\image\image-20211126232002793.png" alt="image-20211126232002793" style="zoom:33%;" /><img src="G:\Tableau\image\image-20211126232019456.png" alt="image-20211126232019456" style="zoom:33%;" />



<img src="G:\Tableau\image\image-20211126232057569.png" alt="image-20211126232057569" style="zoom:40%;" />

对比surf和sift算子进行特征匹配的效果发现差别并不大。

- **场景影响**

分别在室内的场景和室外的场景进行测试，对比效果。

宿舍内、使用surf算子：

<img src="G:\Tableau\image\image-20211126232409335.png" alt="image-20211126232409335" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232431231.png" alt="image-20211126232431231" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232503167.png" alt="image-20211126232503167" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232518613.png" alt="image-20211126232518613" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232535535.png" alt="image-20211126232535535" style="zoom:28%;" />

<img src="G:\Tableau\image\image-20211126232654911.png" alt="image-20211126232654911" style="zoom:40%;" />

宿舍内、使用sift算子：

<img src="G:\Tableau\image\image-20211126232725291.png" alt="image-20211126232725291" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232756483.png" alt="image-20211126232756483" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232817627.png" alt="image-20211126232817627" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232834967.png" alt="image-20211126232834967" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126232852618.png" alt="image-20211126232852618" style="zoom:28%;" />

<img src="G:\Tableau\image\image-20211126232944632.png" alt="image-20211126232944632" style="zoom:40%;" />

球场、使用surf算子：

<img src="G:\Tableau\image\image-20211126233109150.png" alt="image-20211126233109150" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233150178.png" alt="image-20211126233150178" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233209359.png" alt="image-20211126233209359" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233228965.png" alt="image-20211126233228965" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233251243.png" alt="image-20211126233251243" style="zoom:28%;" />

<img src="G:\Tableau\image\image-20211126233326204.png" alt="image-20211126233326204" style="zoom:40%;" />

球场、使用sift算子：

<img src="G:\Tableau\image\image-20211126233403265.png" alt="image-20211126233403265" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233423604.png" alt="image-20211126233423604" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233440918.png" alt="image-20211126233440918" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233457676.png" alt="image-20211126233457676" style="zoom:28%;" /><img src="G:\Tableau\image\image-20211126233512551.png" alt="image-20211126233512551" style="zoom:28%;" />

<img src="G:\Tableau\image\image-20211126233536083.png" alt="image-20211126233536083" style="zoom:40%;" />

- **相机移动影响**

在拍摄的时候移动相机，得到的结果如下：

<img src="G:\Tableau\image\image-20211128190549018.png" alt="image-20211128190549018" style="zoom:40%;" />

​	可以看到如果在拍摄的过程中移动相机，对效果影响很大，在图片拼接出出现明显的断层。



## 实验总结

​	在本次实验中，我基于opencv实现了多个图像的拼接。图像拼接的大致步骤为：

1. 利用surf或sift算子进行特征检测和特征匹配
1. 根据特征匹配利用函数findHomography计算图像之前的单应矩阵H
1. 利用单应矩阵H对图像进行变形（翘曲）
1. 对变形后的图像进行拼接

​	根据实验要求，我对比了影响拼接效果的多个因素，包括特征匹配、场景几何、相机移位。

1. 使用不同的特征检测匹配方法（surf、sift），通过对比发现，在近景图像（宿舍内）拼接中，sift算子表现稍好一点

   <img src="G:\Tableau\image\image-20211126234422579.png" alt="image-20211126234422579" style="zoom: 33%;" /><img src="G:\Tableau\image\image-20211126234459395.png" alt="image-20211126234459395" style="zoom: 33%;" />

2. 对于不同的场景，总体来说差不多，但可能因为远景图像本身更加模糊，如果一些拼接错位的情况，近景中会更加明显。

3. 如果在拍摄的过程中移动相机，对效果影响很大，在图片拼接出出现明显的断层。
